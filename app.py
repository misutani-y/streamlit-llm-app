import os
from functools import lru_cache

import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()
EXPERT_ROLES = {
# === æŒ‡å®šã®æ¡ä»¶ã‚’æº€ãŸã™é–¢æ•° ===
# ãƒ»ã€Œå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã€ã¨ã€Œãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§ã®é¸æŠå€¤ã€ã‚’å¼•æ•°ã¨ã—ã¦å—ã‘å–ã‚Š
# ãƒ»LLM ã‹ã‚‰ã®å›ç­”ã‚’æˆ»ã‚Šå€¤ã¨ã—ã¦è¿”ã™


def run_expert_llm(user_text: str, selected_role_key: str) -> str:
system_message = EXPERT_ROLES.get(selected_role_key, EXPERT_ROLES[DEFAULT_ROLE_KEY])


prompt = ChatPromptTemplate.from_messages([
("system", system_message),
("human", "{input_text}"),
])


chain = prompt | get_llm() | StrOutputParser()
return chain.invoke({"input_text": user_text})


# === Streamlit UI ===
st.set_page_config(page_title="LangChain Expert App", page_icon="ğŸ¤–", layout="centered")
st.title("ğŸ¤– LangChain Expert App")


with st.expander("ã“ã®ã‚¢ãƒ—ãƒªã«ã¤ã„ã¦ï¼ˆæ¦‚è¦ãƒ»æ“ä½œæ–¹æ³•ï¼‰", expanded=True):
st.markdown(
"""
**æ¦‚è¦**
- å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ LangChain çµŒç”±ã§ LLM ã«æŠ•ã’ã€**å›ç­”ã‚’ç”»é¢ã«è¡¨ç¤º**ã—ã¾ã™ã€‚
- **ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³**ã§å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ï¼ˆA/Bï¼‰ã‚’é¸ã¶ã¨ã€**ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè‡ªå‹•ã§åˆ‡ã‚Šæ›¿ã‚ã‚Š**ã€å›ç­”ã®è¦³ç‚¹ãŒå¤‰ã‚ã‚Šã¾ã™ã€‚


**æ“ä½œæ‰‹é †**
1. å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ï¼ˆA ã¾ãŸã¯ Bï¼‰ã‚’é¸æŠ
2. ä¸‹ã®å…¥åŠ›æ¬„ã«è³ªå•ã‚„æ–‡ç« ã‚’å…¥åŠ›
3. ã€Œé€ä¿¡ã€ã‚’æŠ¼ã™ã¨ã€æ•°ç§’å¾Œã«å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
"""
)


# ãƒ­ãƒ¼ãƒ«é¸æŠï¼ˆãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ï¼‰
role_key = st.radio("å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ã‚’é¸æŠ", options=list(EXPERT_ROLES.keys()), index=0, horizontal=False)


# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
user_text = st.text_area("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ", placeholder="ã“ã“ã«è³ªå•ã‚„æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„â€¦", height=180)


col1, col2 = st.columns([1, 1])
with col1:
submitted = st.button("é€ä¿¡", type="primary")
with col2:
show_prompt = st.toggle("ãƒ‡ãƒãƒƒã‚°: ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º", value=False)


# API ã‚­ãƒ¼ç¢ºèª
if not os.getenv("OPENAI_API_KEY"):
st.warning("OPENAI_API_KEY ãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å®Ÿè¡Œå‰ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚", icon="âš ï¸")


# å®Ÿè¡Œ
if submitted:
if not user_text.strip():
st.error("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
else:
try:
if show_prompt:
with st.expander("å®Ÿéš›ã«ä½¿ã‚ã‚Œã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", expanded=False):
st.code(EXPERT_ROLES[role_key], language="markdown")


with st.spinner("LLM ã«å•ã„åˆã‚ã›ä¸­â€¦"):
answer = run_expert_llm(user_text, role_key)


st.subheader("å›ç­”")
st.markdown(answer)
except Exception as e:
st.exception(e)

print(result.content)