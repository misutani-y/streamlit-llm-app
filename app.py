import os
from pathlib import Path
from functools import lru_cache

import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()
EXPERT_ROLES = {
    "ä¿é™ºãƒ»ãƒ©ã‚¤ãƒ•ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®å°‚é–€å®¶": """
ã‚ãªãŸã¯æ—¥æœ¬ã®ä¿é™ºãƒ»ãƒ©ã‚¤ãƒ•ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°é ˜åŸŸã®å°‚é–€å®¶ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çŠ¶æ³ã‚’å¼•ãå‡ºã—ã¤ã¤ã€
1) å‰ææ•´ç† 2) é¸æŠè‚¢æç¤ºï¼ˆé•·çŸ­æ‰€ï¼‰3) å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ 4) æ³¨æ„ç‚¹ï¼ˆæ³•ä»¤/ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ï¼‰
ã®é †ã§ã€ã‚ã‹ã‚Šã‚„ã™ãæ—¥æœ¬èªã§åŠ©è¨€ã—ã¦ãã ã•ã„ã€‚å¿…è¦ãªã‚‰å…¬çš„ä¿é™ºåˆ¶åº¦ã‚‚å¹³æ˜“ã«èª¬æ˜ã—ã¾ã™ã€‚
""",
    "AIé–‹ç™ºã®å°‚é–€å®¶": """
ã‚ãªãŸã¯AI/LLMã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã§ã™ã€‚LangChainã‚„RAGã€è©•ä¾¡ãƒ»é‹ç”¨ã¾ã§ç²¾é€šã—ã¦ã„ã¾ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èª²é¡Œã«å¯¾ã—ã€
1) ç›®çš„ã®æ˜ç¢ºåŒ– 2) æ¨å¥¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ 3) æ‰‹é †/ã‚³ãƒ¼ãƒ‰ä¾‹ 4) ãƒªã‚¹ã‚¯ãƒ»ä»£æ›¿æ¡ˆ
ã‚’ã€ç®‡æ¡æ›¸ãä¸­å¿ƒã§æ—¥æœ¬èªç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚å¿…è¦ã«å¿œã˜ã¦ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚‚ç¤ºã—ã¾ã™ã€‚
"""
}
DEFAULT_ROLE_KEY = "ä¿é™ºãƒ»ãƒ©ã‚¤ãƒ•ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®å°‚é–€å®¶"


def run_expert_llm(user_text: str, selected_role_key: str) -> str:
    system_message = EXPERT_ROLES.get(selected_role_key, EXPERT_ROLES[DEFAULT_ROLE_KEY])

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        ("human", "{input_text}"),
    ])

    chain = prompt | get_llm() | StrOutputParser()
    return chain.invoke({"input_text": user_text})

@lru_cache(maxsize=1)
def get_llm():
    return ChatOpenAI(temperature=0.7, model_name="gpt-4o-mini")

# === Streamlit UI ===
st.set_page_config(page_title="å°‚é–€å®¶ã«èã„ã¦ã¿ã‚ˆã†", page_icon="ğŸ¤–", layout="centered")
st.title("ğŸ¤– å°‚é–€å®¶ã«èã„ã¦ã¿ã‚ˆã†")

with st.expander("ã“ã®ã‚¢ãƒ—ãƒªã«ã¤ã„ã¦ï¼ˆæ¦‚è¦ãƒ»æ“ä½œæ–¹æ³•ï¼‰", expanded=True):
    st.markdown(
        """
        **æ¦‚è¦**
        - **ãƒœã‚¿ãƒ³**ã§å°‚é–€å®¶ã‚’é¸ã¶ã¨ã€**ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè‡ªå‹•ã§åˆ‡ã‚Šæ›¿ã‚ã‚Š**ã€å›ç­”ã®è¦³ç‚¹ãŒå¤‰ã‚ã‚Šã¾ã™ã€‚

        **æ“ä½œæ‰‹é †**
        1. å°‚é–€å®¶ã‚’é¸æŠ
        2. ä¸‹ã®å…¥åŠ›æ¬„ã«è³ªå•ã‚„æ–‡ç« ã‚’å…¥åŠ›
        3. ã€Œé€ä¿¡ã€ã‚’æŠ¼ã™ã¨ã€æ•°ç§’å¾Œã«å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
        """
    )

# ãƒ­ãƒ¼ãƒ«é¸æŠï¼ˆãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ï¼‰
role_key = st.radio("å°‚é–€å®¶ã‚’é¸æŠ", options=list(EXPERT_ROLES.keys()), index=0, horizontal=False)

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
user_text = st.text_area("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ", placeholder="ã“ã“ã«è³ªå•ã‚„æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„â€¦", height=180)

col1, col2 = st.columns([1, 1])
with col1:
    submitted = st.button("é€ä¿¡", type="primary")
with col2:
    show_prompt = st.toggle("ãƒ‡ãƒãƒƒã‚°: ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º", value=False)

# API ã‚­ãƒ¼ç¢ºèª
if not os.getenv("OPENAI_API_KEY"):
    st.warning("OPENAI_API_KEY ãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å®Ÿè¡Œå‰ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚", icon="âš ï¸")

# å®Ÿè¡Œ
if submitted:
    if not user_text.strip():
        st.error("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        try:
            if show_prompt:
                with st.expander("å®Ÿéš›ã«ä½¿ã‚ã‚Œã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", expanded=False):
                    st.code(EXPERT_ROLES[role_key], language="markdown")

            with st.spinner("LLM ã«å•ã„åˆã‚ã›ä¸­â€¦"):
                answer = run_expert_llm(user_text, role_key)

            st.subheader("å›ç­”")
            st.markdown(answer)
        except Exception as e:
            st.exception(e)

# print(result.content) # This line was causing an error as 'result' was not defined
